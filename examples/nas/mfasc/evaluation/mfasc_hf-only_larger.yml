general:
    worker:
        timeout: 1000.0

#pipeline: [nas]
pipeline: [nas, fully_train]
#pipeline: [fully_train]

nas:
    pipe_step:
        type: NasPipeStepMF
        seed: 0

    dataset:
        type: Cifar10
        common:
            num_workers: 0
            data_path: '~/misc/cache/datasets/cifar10/'
            download: True
        train:
            batch_size: 64
            train_portion: 0.1
            cutout_length: 16
        # TODO: fix soon 
        test:
            batch_size: 2048
            train_portion: 0.1

    search_algorithm:
        type: MFASC
        max_budget: 500
        hf_epochs: 10
        lf_epochs: 3
        fidelity_ratio: 1
        min_hf_sample_size: 10
        min_lf_sample_size: 2
        seed: 0

    search_space:
        type: SearchSpace
        modules: ['custom']
        custom:
            name: MobileNetV2
            num_classes: 10
            layer_0:
                repetitions: [1, 2, 3, 4]
                channels: [16, 24]
            layer_1:
                repetitions: [1, 2, 3, 4]
                channels: [24, 32]
            layer_2:
                repetitions: [1, 2, 3, 4]
                channels: [32, 64]
            layer_3:
                repetitions: [1, 2, 3, 4]
                channels: [64, 96]
            layer_4:
                repetitions: [1, 2, 3, 4]
                channels: [96, 160]
            layer_5:
                repetitions: [1, 2, 3, 4]
                channels: [160, 320]
            layer_6:
                repetitions: [1, 2, 3, 4]
                channels: [320, 640]

    trainer:
      type: Trainer
      epochs: 1
      valid_freq: 1
      optim:
          type: SGD
          lr: 0.001
          momentum: 0.9
          weight_decay: 5.0e-4


fully_train:
    pipe_step:
        type: FullyTrainPipeStep

    dataset:
        type: Cifar10
        common:
            data_path: '~/misc/cache/datasets/cifar10/'
            download: True
        train:
            shuffle: False
            num_workers: 8
            batch_size: 128
            train_portion: 1.0
        test:
            shuffle: False
            num_workers: 8
            batch_size: 128
    model:
        #model_desc_file: '{local_base_path}/best_model_desc.json'
        model_desc_file: '{local_base_path}/best_model_desc.json'

    trainer:
        type: Trainer
        epochs: 20
        valid_freq: 5
        optim:
            type: SGD
            lr: 0.1
            momentum: 0.9
            weight_decay: 5.0e-4
        lr_scheduler:
            type: CosineAnnealingLR
            T_max: 100